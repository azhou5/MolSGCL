# MolSGCL
Datasets for molecular property prediction are small relative to the vast chemical space, making generalization from limited experiments a central challenge. We present Mol-SGCL, a method that shapes the latent space of molecular property prediction models to align with science-based priors. We hypothesize that engineering inductive biases directly into the representation space encourages models to learn chemical principles rather than spurious shortcuts. Concretely, Mol-SGCL employs a triplet loss that pulls a molecule’s representation toward representations of plausibly causal substructures and pushes it away from implausibly causal ones. Plausibility is either defined by domain-specific rules in Mol-SGCL_Riules or by a large language model in Mol-SGCL_LLM. To stress-test out-of-distribution (OOD) generalization under data scarcity, we construct modified Therapeutics Data Commons tasks that minimize train–test similarity and cap the training set at 150 molecules. On these OOD splits, both Mol-SGCL_Rules and MolSGCL_LLM consistently outperform baselines, suggesting that \name shapes the representation space to learn invariant features and improves model generalizability in data-constrained regimes. We show that this framework successfully transfers to Minimol, a state-of-the-art molecular property prediction method, suggesting that it is not constrained to a specific model design. We further envision extending \name beyond molecular property prediction to any setting where inputs can be decomposed into substructures whose presence, absence, or configuration has a causal influence on the target label. 
